{% if make_parse_tree -%}
from dataclasses import dataclass
{%- endif %}
{%- if memoize %}
{%- if make_parse_tree %}
from typing import Any, Callable, Dict, List, Optional, Tuple, Union
{%- else %}
from typing import Any, Callable, Dict, List, Optional, Tuple
{% endif -%}
{%- else %}
{%- if make_parse_tree %}
from typing import List, Optional, Union
{%- else %}
from typing import List, Optional
{%- endif %}
{%- endif %}

{% if make_parse_tree -%}
from .tokens import Token, TokenType, Tokenizer, {{ ret_type }}


@dataclass
class ParserNode:
    nodes: List[Union[Optional[{{ ret_type }}], List[{{ ret_type }}]]]
{% else -%}
from .tokens import Token, TokenType, Tokenizer
{% endif %}

class _Parser:
    """Parser base class containing helper functions"""

    def __init__(self, tokenizer: Tokenizer):
        self._tok = tokenizer
        self.pos = -1
        self._tokens: List[Token] = []
        self._next_token()

    def _curr_token(self) -> Token:
        return self._tokens[self.pos]

    def _next_token(self) -> Token:
        self.pos += 1

        if self.pos < len(self._tokens):
            return self._tokens[self.pos]

        tok = self._tok.next_token()
        self._tokens.append(tok)
        return tok

    def _match_token_or_rollback(self, tt: TokenType, old_pos: int) -> Optional[Token]:
        tok = self._curr_token()

        if tok.token_type != tt:
            self.pos = old_pos
            return None

        self._next_token()
        return tok

    def _match_tokens_or_rollback(self, tt: TokenType, old_pos: int) -> List[Token]:
        token = self._match_token_or_rollback(tt, old_pos)
        if not token:
            self.pos = old_pos
            return []

        tokens = self._try_match_tokens(tt)
        return [token, *tokens]

    def _try_match_token(self, tt: TokenType) -> Optional[Token]:
        tok = self._curr_token()

        if tok.token_type != tt:
            return None

        self._next_token()
        return tok

    def _try_match_tokens(self, tt: TokenType) -> List[Token]:
        tokens: List[Token] = []
        
        while True:
            tok = self._try_match_token(tt)
            if not tok:
                break
            tokens.append(tok)

        return tokens

{% if memoize %}
def _memoize(func):
    def memoize_wrapper(self):
        pos = self.pos
        key = (func, pos)

        if key in self._memos:
            result, new_pos = self._memos[key]
            self.pos = new_pos
            return result

        result = func(self)
        key = (func, pos)
        self._memos[key] = result, self.pos
        return result

    return memoize_wrapper
{% endif %}

class {{ name }}Parser(_Parser):
    """Primary parser class"""

    def __init__(self, tokenizer: Tokenizer):
        super().__init__(tokenizer)
{%- if memoize %}
        self._memos: Dict[Tuple[Callable, int], Any] = {}
{%- endif %}

{% for func in functions %}
{% if memoize %}    @_memoize{% endif %}
{{ func }}
{% endfor %}
